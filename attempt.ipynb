{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from overrides import overrides\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archai.discrete_search.api import ArchaiModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,nb_layers: int,kernel_size: int,hidden_dim:int):\n",
    "        super().__init__()\n",
    "        self.nb_layers=nb_layers\n",
    "        self.kernel_size=kernel_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        layer_list = []\n",
    "\n",
    "        for i in range(nb_layers):\n",
    "            in_ch = (1 if i == 0 else hidden_dim)\n",
    "\n",
    "            layer_list += [\n",
    "                nn.Conv2d(in_ch, hidden_dim, kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "\n",
    "        layer_list += [\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Conv2d(hidden_dim, 10, kernel_size=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layer_list)\n",
    "    def forward(self,x):\n",
    "        return self.model(x).squeeze()\n",
    "    def get_archid(self):\n",
    "        return f'({self.nb_layers}, {self.kernel_size}, {self.hidden_dim})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj=MyModel(3,3,32)\n",
    "model=ArchaiModel(arch=model_obj,archid=f'L={model_obj.nb_layers}, K={model_obj.kernel_size}, H={model_obj.hidden_dim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L=3, K=3, H=32'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.archid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (10): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import Random\n",
    "from archai.discrete_search.api import DiscreteSearchSpace\n",
    "\n",
    "import json\n",
    "from random import Random\n",
    "from archai.discrete_search.api import DiscreteSearchSpace\n",
    "\n",
    "\n",
    "class CNNSearchSpace(DiscreteSearchSpace):\n",
    "    def __init__(self, min_layers: int = 1, max_layers: int = 12,\n",
    "                 kernel_list=(1, 3, 5, 7), hidden_list=(16, 32, 64, 128),\n",
    "                 seed: int = 1):\n",
    "\n",
    "        self.min_layers = min_layers\n",
    "        self.max_layers = max_layers\n",
    "        self.kernel_list = kernel_list\n",
    "        self.hidden_list = hidden_list\n",
    "\n",
    "        self.rng = Random(seed)\n",
    "\n",
    "    def get_archid(self, model: MyModel) -> str:\n",
    "        return f'L={model.nb_layers}, K={model.kernel_size}, H={model.hidden_dim}'\n",
    "\n",
    "    @overrides\n",
    "    def random_sample(self) -> ArchaiModel:\n",
    "        # Randomly chooses architecture parameters\n",
    "        nb_layers = self.rng.randint(1, self.max_layers)\n",
    "        kernel_size = self.rng.choice(self.kernel_list)\n",
    "        hidden_dim = self.rng.choice(self.hidden_list)\n",
    "\n",
    "        model = MyModel(nb_layers, kernel_size, hidden_dim)\n",
    "\n",
    "        # Wraps model into ArchaiModel\n",
    "        return ArchaiModel(arch=model, archid=self.get_archid(model))\n",
    "\n",
    "    @overrides\n",
    "    def save_arch(self, model: ArchaiModel, file_path: str=r'C:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\ff_test.json'):\n",
    "        with open(file_path, 'w') as fp:\n",
    "            json.dump({\n",
    "                'nb_layers': model.arch.nb_layers,\n",
    "                'kernel_size': model.arch.kernel_size,\n",
    "                'hidden_dim': model.arch.hidden_dim\n",
    "            }, fp)\n",
    "\n",
    "    @overrides\n",
    "    def load_arch(self, file_path: str)->ArchaiModel:\n",
    "        config = json.load(open(file_path))\n",
    "        model = MyModel(**config)\n",
    "\n",
    "        return ArchaiModel(arch=model, archid=self.get_archid(model))\n",
    "\n",
    "    @overrides\n",
    "    def save_model_weights(self, model: ArchaiModel, file_path: str=r'C:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\ff_test.json'):\n",
    "        state_dict = model.arch.get_state_dict()\n",
    "        torch.save(state_dict, file_path)\n",
    "\n",
    "    @overrides\n",
    "    def load_model_weights(self, model: ArchaiModel, file_path: str=r'C:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\ff_test.json'):\n",
    "        model.arch.load_state_dict(torch.load(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archai.discrete_search.api.search_space import EvolutionarySearchSpace, BayesOptSearchSpace\n",
    "from random import random\n",
    "class CNNSearchSpaceExt(CNNSearchSpace, EvolutionarySearchSpace, BayesOptSearchSpace):\n",
    "    ''' We are subclassing CNNSearchSpace just to save up space'''\n",
    "\n",
    "    @overrides\n",
    "    def mutate(self, arch: ArchaiModel) -> ArchaiModel:\n",
    "        print(arch.arch.nb_layers)\n",
    "        self.config = {\n",
    "            'nb_layers': arch.arch.nb_layers,\n",
    "            'kernel_size': arch.arch.kernel_size,\n",
    "            'hidden_dim': arch.arch.hidden_dim\n",
    "        }\n",
    "        print(self.config)\n",
    "        if random() < 0.2:\n",
    "            self.config['nb_layers'] = self.rng.randint(self.min_layers, self.max_layers)\n",
    "\n",
    "        if random() < 0.2:\n",
    "            self.config['kernel_size'] = self.rng.choice(self.kernel_list)\n",
    "\n",
    "        if random() < 0.2:\n",
    "            self.config['hidden_dim'] = self.rng.choice(self.hidden_list)\n",
    "\n",
    "        mutated_model = MyModel(**self.config)\n",
    "\n",
    "        return ArchaiModel(\n",
    "            arch=mutated_model, archid=self.get_archid(mutated_model)\n",
    "        )\n",
    "\n",
    "    @overrides\n",
    "    def crossover(self, arch_list: List[ArchaiModel]) -> ArchaiModel:\n",
    "        new_config = {\n",
    "            'nb_layers': self.rng.choice([m.arch.nb_layers for m in arch_list]),\n",
    "            'kernel_size': self.rng.choice([m.arch.kernel_size for m in arch_list]),\n",
    "            'hidden_dim': self.rng.choice([m.arch.hidden_dim for m in arch_list]),\n",
    "        }\n",
    "\n",
    "        crossover_model = MyModel(**new_config)\n",
    "\n",
    "        return ArchaiModel(\n",
    "            arch=crossover_model, archid=self.get_archid(crossover_model)\n",
    "        )\n",
    "\n",
    "    @overrides\n",
    "    def encode(self,arch: ArchaiModel) -> np.ndarray:\n",
    "        return np.array([arch.nb_layers,arch.kernel_size, arch.hidden_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ss = CNNSearchSpaceExt(max_layers=10, kernel_list=[3, 5, 7], hidden_list=[16, 32, 64])\n",
    "m=ss.random_sample()\n",
    "print(m.arch.nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=3, K=7, H=16\n",
      "5\n",
      "{'nb_layers': 5, 'kernel_size': 3, 'hidden_dim': 32}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArchaiModel(\n",
       "\tarchid=L=5, K=5, H=32, \n",
       "\tmetadata={}, \n",
       "\tarch=MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (16): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ss.random_sample()\n",
    "print(m.archid) \n",
    "ss.mutate(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'nb_layers': 3, 'kernel_size': 7, 'hidden_dim': 16}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "m1=ss.mutate(m)\n",
    "print(m1.arch.nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a configsearchspace for initial experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1000])\n",
      "1331472\n",
      "torch.Size([5, 1000])\n",
      "2382944\n",
      "torch.Size([5, 1000])\n",
      "5636720\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "\n",
    "        # Local representations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Global representations\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
    "        x = self.transformer(x)\n",
    "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)\n",
    "\n",
    "        # Fusion\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))   # Repeat\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "        \n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0]*2)))\n",
    "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1]*4)))\n",
    "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2]*4)))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih//32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mv2[0](x)\n",
    "\n",
    "        x = self.mv2[1](x)\n",
    "        x = self.mv2[2](x)\n",
    "        x = self.mv2[3](x)      # Repeat\n",
    "\n",
    "        x = self.mv2[4](x)\n",
    "        x = self.mvit[0](x)\n",
    "\n",
    "        x = self.mv2[5](x)\n",
    "        x = self.mvit[1](x)\n",
    "\n",
    "        x = self.mv2[6](x)\n",
    "        x = self.mvit[2](x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilevit_xxs():\n",
    "    dims = [64, 80, 96]\n",
    "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
    "\n",
    "\n",
    "def mobilevit_xs():\n",
    "    dims = [96, 120, 144]\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def mobilevit_s():\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = torch.randn(5, 3, 256, 256)\n",
    "\n",
    "    vit = mobilevit_xxs()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n",
    "\n",
    "    vit = mobilevit_xs()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n",
    "\n",
    "    vit = mobilevit_s()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Objectives + Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TorchFlops' from 'archai.discrete_search.evaluators' (c:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\archai\\discrete_search\\evaluators\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39marchai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiscrete_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m SearchObjectives\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39marchai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiscrete_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluators\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchFlops\n\u001b[0;32m      3\u001b[0m objectives \u001b[39m=\u001b[39m SearchObjectives()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TorchFlops' from 'archai.discrete_search.evaluators' (c:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\archai\\discrete_search\\evaluators\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from archai.discrete_search.api import SearchObjectives\n",
    "from archai.discrete_search.evaluators import TorchFlops\n",
    "objectives = SearchObjectives()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Objective, in this case Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archai.discrete_search.evaluators.pt_profiler import TorchFlops\n",
    "ss = CNNSearchSpaceExt(max_layers=10, kernel_list=[3, 5, 7], hidden_list=[16, 32, 64])\n",
    "objectives = SearchObjectives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arch=model\n",
    "objectives.add_objective(\n",
    "    'FLOPs', TorchFlops(torch.randn(1, 1, 28, 28)),\n",
    "    higher_is_better=False,\n",
    "    compute_intensive=False,\n",
    "    # We may optionally add a constraint.\n",
    "    # Architectures outside this range will be ignored by the search algorithm\n",
    "    # constraint=(0.0, 1e9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archai.discrete_search.algos import EvolutionParetoSearch\n",
    "algo = EvolutionParetoSearch(\n",
    "    ss, objectives,\n",
    "    output_dir='./out_evo',\n",
    "    num_iters=5, num_crossovers=5,\n",
    "    mutations_per_parent=5,\n",
    "    max_unseen_population=10,\n",
    "    save_pareto_model_weights=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 12:18:28,997 - archai.discrete_search.algos.evolution_pareto — INFO —  Using 10 random architectures as the initial population ...\n",
      "2023-07-04 12:18:29,016 - archai.discrete_search.algos.evolution_pareto — INFO —  Iteration 1/5\n",
      "2023-07-04 12:18:29,018 - archai.discrete_search.algos.evolution_pareto — INFO —  Calculating search objectives ['FLOPs'] for 10 models ...\n",
      "2023-07-04 12:18:29,094 - archai.discrete_search.algos.evolution_pareto — INFO —  Updating Pareto frontier ...\n",
      "2023-07-04 12:18:29,095 - archai.discrete_search.algos.evolution_pareto — INFO —  Found 1 members.\n",
      "2023-07-04 12:18:29,101 - archai.discrete_search.algos.evolution_pareto — INFO —  Optimzing memory usage ...\n",
      "2023-07-04 12:18:29,104 - archai.discrete_search.algos.evolution_pareto — INFO —  Choosing 1 parents ...\n",
      "2023-07-04 12:18:29,105 - archai.discrete_search.algos.evolution_pareto — INFO —  wtf man\n",
      "2023-07-04 12:18:29,106 - archai.discrete_search.algos.evolution_pareto — INFO —  L=1, K=7, H=64\n",
      "2023-07-04 12:18:29,107 - archai.discrete_search.algos.evolution_pareto — INFO —  Crossover: 0 new models.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mutated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search_results \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39msearch()\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\Downloads\\archai-main\\archai-main\\archai\\discrete_search\\algos\\evolution_pareto.py:303\u001b[0m, in \u001b[0;36mEvolutionParetoSearch.search\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39m# sample some random samples to add to the parent mix\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39m# to mitigage local minima\u001b[39;00m\n\u001b[0;32m    302\u001b[0m rand_mix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_models(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_random_mix)\n\u001b[1;32m--> 303\u001b[0m unseen_pop \u001b[39m=\u001b[39m crossovered \u001b[39m+\u001b[39m mutated \u001b[39m+\u001b[39m rand_mix\n\u001b[0;32m    305\u001b[0m \u001b[39m# shuffle before we pick a smaller population for the next stage\u001b[39;00m\n\u001b[0;32m    306\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal unseen population: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(unseen_pop)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mutated' is not defined"
     ]
    }
   ],
   "source": [
    "search_results = algo.search()\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CNNSearchSpace"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
